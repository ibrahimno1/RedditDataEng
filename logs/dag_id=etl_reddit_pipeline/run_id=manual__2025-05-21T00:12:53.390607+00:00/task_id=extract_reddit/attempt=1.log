[2025-05-21T00:12:54.036+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.extract_reddit manual__2025-05-21T00:12:53.390607+00:00 [queued]>
[2025-05-21T00:12:54.040+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.extract_reddit manual__2025-05-21T00:12:53.390607+00:00 [queued]>
[2025-05-21T00:12:54.040+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-05-21T00:12:54.047+0000] {taskinstance.py:1380} INFO - Executing <Task(PythonOperator): extract_reddit> on 2025-05-21 00:12:53.390607+00:00
[2025-05-21T00:12:54.049+0000] {standard_task_runner.py:57} INFO - Started process 61 to run task
[2025-05-21T00:12:54.051+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'extract_reddit', 'manual__2025-05-21T00:12:53.390607+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpq92jvaay']
[2025-05-21T00:12:54.052+0000] {standard_task_runner.py:85} INFO - Job 24: Subtask extract_reddit
[2025-05-21T00:12:54.073+0000] {task_command.py:415} INFO - Running <TaskInstance: etl_reddit_pipeline.extract_reddit manual__2025-05-21T00:12:53.390607+00:00 [running]> on host 0bdedd2b68b9
[2025-05-21T00:12:54.108+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Ibrahim' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='extract_reddit' AIRFLOW_CTX_EXECUTION_DATE='2025-05-21T00:12:53.390607+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-21T00:12:53.390607+00:00'
[2025-05-21T00:12:54.109+0000] {logging_mixin.py:151} INFO - Connected to Reddit API successfully.
[2025-05-21T00:12:54.976+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Which Editor do you use to write SQL code. And does that differ for the different flavours of SQL.\n\nI nowadays try to use vim dadbod or vscode with extensions.\n\n', 'author_fullname': 't2_1oj0e2xwke', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Which SQL editor do you use?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krb45d', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 45, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 45, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747762884.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Which Editor do you use to write SQL code. And does that differ for the different flavours of SQL.</p>\n\n<p>I nowadays try to use vim dadbod or vscode with extensions.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1krb45d', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Hungry_Ad8053'), 'discussion_type': None, 'num_comments': 86, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krb45d/which_sql_editor_do_you_use/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krb45d/which_sql_editor_do_you_use/', 'subreddit_subscribers': 328156, 'created_utc': 1747762884.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.977+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Data engineering has so much potential in everyday life, but it takes effort. Who’s working on a side project/hobby/hustle that you’re willing to share?', 'author_fullname': 't2_twfbg', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone working on cool side projects?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krcm3p', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747766423.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Data engineering has so much potential in everyday life, but it takes effort. Who’s working on a side project/hobby/hustle that you’re willing to share?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1krcm3p', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='alexstrehlke'), 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krcm3p/anyone_working_on_cool_side_projects/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krcm3p/anyone_working_on_cool_side_projects/', 'subreddit_subscribers': 328156, 'created_utc': 1747766423.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.977+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In the last few weeks i am low at creativity, i am no learning anything or doing enough efforts, i feel emptiness at my job rn as a DE, i am not capable of completing tasks on schedule, or solving problems by myself instead everytime someone needs to step in and give me a hand or solve it while i am watching like some idiot\n\nBefore this period, i was super creative, solving crazy problems, fast on schedule, and required minimum help from my collegues, and very motivated\n\nIf anyone passed from this situation can share his experience', 'author_fullname': 't2_19zooibc4s', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Passing from a empty period, with low creativity as a DE', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr6009', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747750677.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In the last few weeks i am low at creativity, i am no learning anything or doing enough efforts, i feel emptiness at my job rn as a DE, i am not capable of completing tasks on schedule, or solving problems by myself instead everytime someone needs to step in and give me a hand or solve it while i am watching like some idiot</p>\n\n<p>Before this period, i was super creative, solving crazy problems, fast on schedule, and required minimum help from my collegues, and very motivated</p>\n\n<p>If anyone passed from this situation can share his experience</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1kr6009', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='HMZ_PBI'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr6009/passing_from_a_empty_period_with_low_creativity/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr6009/passing_from_a_empty_period_with_low_creativity/', 'subreddit_subscribers': 328156, 'created_utc': 1747750677.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.977+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Right after graduating, I landed a role as a DBA/Data Engineer at a small but growing company. Until last year, they had been handling data through file shares until they had a consultancy company build them Synapse workspace with daily data refreshes. While I was initially just desperate to get my foot in the door, I’ve genuinely come to enjoy this role and the challenges that come with it. I am the only one working as a DE and while my manager is somewhat knowledgeable in IT space, I can't truly consider him as my DE mentor. That said, I was pretty much thrown into the deep end, and while I’ve learned a lot through trial and error, I do wish that I had started under a senior who could be a mentor for me.\n\nFiguring out things myself has sort of a double edge, where on one hand, the process of figuring out has sometimes lead to new learning endeavours while sometimes I'm just left wondering: Is this really the optimal solution? \n\nSo, I’m hoping to get some advice from this community:\n\n# 1. Mentorship & Guidance\n\n* How did you find a mentor (internally or externally)?\n* Are there communities (Slack, Discord, forums) you’d recommend joining?\n* Are there folks in the data space worth following (blogs, LinkedIn, GitHub, etc.)? I currenlty follow Zack wilson and a few others who can be found by surface level research into the space.\n\n# 2. Conferences & Meetups\n\n* Have any of you found value in attending data engineering or analytics conferences?\n* Any recommendations for events that are beginner-friendly and actually useful for someone in a role like mine?\n\n# 3. Improving as a Solo Data Engineer\n\n* Any learning paths or courses that helped you understand more than just what works but also *w*hy?", 'author_fullname': 't2_ii76jky0b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Early-career Data Engineer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr4jcg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.73, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747746774.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Right after graduating, I landed a role as a DBA/Data Engineer at a small but growing company. Until last year, they had been handling data through file shares until they had a consultancy company build them Synapse workspace with daily data refreshes. While I was initially just desperate to get my foot in the door, I’ve genuinely come to enjoy this role and the challenges that come with it. I am the only one working as a DE and while my manager is somewhat knowledgeable in IT space, I can&#39;t truly consider him as my DE mentor. That said, I was pretty much thrown into the deep end, and while I’ve learned a lot through trial and error, I do wish that I had started under a senior who could be a mentor for me.</p>\n\n<p>Figuring out things myself has sort of a double edge, where on one hand, the process of figuring out has sometimes lead to new learning endeavours while sometimes I&#39;m just left wondering: Is this really the optimal solution? </p>\n\n<p>So, I’m hoping to get some advice from this community:</p>\n\n<h1>1. Mentorship &amp; Guidance</h1>\n\n<ul>\n<li>How did you find a mentor (internally or externally)?</li>\n<li>Are there communities (Slack, Discord, forums) you’d recommend joining?</li>\n<li>Are there folks in the data space worth following (blogs, LinkedIn, GitHub, etc.)? I currenlty follow Zack wilson and a few others who can be found by surface level research into the space.</li>\n</ul>\n\n<h1>2. Conferences &amp; Meetups</h1>\n\n<ul>\n<li>Have any of you found value in attending data engineering or analytics conferences?</li>\n<li>Any recommendations for events that are beginner-friendly and actually useful for someone in a role like mine?</li>\n</ul>\n\n<h1>3. Improving as a Solo Data Engineer</h1>\n\n<ul>\n<li>Any learning paths or courses that helped you understand more than just what works but also <em>w</em>hy?</li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1kr4jcg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='NoIntroduction9767'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr4jcg/earlycareer_data_engineer/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr4jcg/earlycareer_data_engineer/', 'subreddit_subscribers': 328156, 'created_utc': 1747746774.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.978+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "* [Add a new Ollama Processor as built-in processor](https://github.com/ConduitIO/conduit/pull/2227).\n* [Add the ability to easily configure Processor Plugins when building a custom Conduit](https://github.com/ConduitIO/conduit/pull/2260). Check out the\xa0[documentation page for more information](https://conduit.io/docs/using/processors/additional-built-in-plugins). Thanks\xa0[u/nickchomey](https://github.com/nickchomey)\xa0for the contribution!\n* [New configuration option\xa0`preview.pipeline-arch-v2-disable-metrics`\xa0to disable metrics for the new pipeline architecture](https://github.com/ConduitIO/conduit/pull/2271).\n* [Fixes a bug where the ENV variable\xa0`CONDUIT_CONFIG_PATH`\xa0didn't seem to work propertly](https://github.com/ConduitIO/ecdysis/issues/25).\n* [Fixes a bug when using the default processor middleware](https://github.com/ConduitIO/conduit-processor-sdk/issues/111).", 'author_fullname': 't2_22mnhc82', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Conduit v0.13.5 with a new Ollama processor', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 46, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr78ax', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/O7-F2HprvvMttKb6ErPbvEuF-Jt2LxVeTczXeqSNM0M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747753683.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'conduit.io', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><ul>\n<li><a href="https://github.com/ConduitIO/conduit/pull/2227">Add a new Ollama Processor as built-in processor</a>.</li>\n<li><a href="https://github.com/ConduitIO/conduit/pull/2260">Add the ability to easily configure Processor Plugins when building a custom Conduit</a>. Check out the\xa0<a href="https://conduit.io/docs/using/processors/additional-built-in-plugins">documentation page for more information</a>. Thanks\xa0<a href="https://github.com/nickchomey">u/nickchomey</a>\xa0for the contribution!</li>\n<li><a href="https://github.com/ConduitIO/conduit/pull/2271">New configuration option\xa0<code>preview.pipeline-arch-v2-disable-metrics</code>\xa0to disable metrics for the new pipeline architecture</a>.</li>\n<li><a href="https://github.com/ConduitIO/ecdysis/issues/25">Fixes a bug where the ENV variable\xa0<code>CONDUIT_CONFIG_PATH</code>\xa0didn&#39;t seem to work propertly</a>.</li>\n<li><a href="https://github.com/ConduitIO/conduit-processor-sdk/issues/111">Fixes a bug when using the default processor middleware</a>.</li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://conduit.io/changelog/2025-05-20-conduit-0-13-5-release', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?auto=webp&s=26edf2d6790d79874ed788140fb71dbf637ac721', 'width': 1500, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=108&crop=smart&auto=webp&s=a096933df04315cff0cc4653e373c475e4fdecec', 'width': 108, 'height': 36}, {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=216&crop=smart&auto=webp&s=b009c4acfc95cd8406794527ee0b06aff07a5865', 'width': 216, 'height': 72}, {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=320&crop=smart&auto=webp&s=c724f202f893c77f64863428c956c81a9c304345', 'width': 320, 'height': 106}, {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=640&crop=smart&auto=webp&s=7cc4d7353be2d6f5cb50b44144a0f08bf525f53b', 'width': 640, 'height': 213}, {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=960&crop=smart&auto=webp&s=964bb80ed680d5c8319d733c475b7cfd34bb18b9', 'width': 960, 'height': 320}, {'url': 'https://external-preview.redd.it/CV0jSaV0ACPdqEho3tZGcZQwX4hHVQeg-2w0CqKD2gI.jpg?width=1080&crop=smart&auto=webp&s=f1b5febb36466414386ca6e45bac1bd9989a411b', 'width': 1080, 'height': 360}], 'variants': {}, 'id': '4xuB92CZga7WemhFK7eqOoY3imVhcVoXEyv2RaFhyyg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1kr78ax', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='raulb_'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr78ax/conduit_v0135_with_a_new_ollama_processor/', 'stickied': False, 'url': 'https://conduit.io/changelog/2025-05-20-conduit-0-13-5-release', 'subreddit_subscribers': 328156, 'created_utc': 1747753683.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.978+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have quite a complex SQL query within DBT which I have been tasked to build an API \'on top of\'.\n\nMore specifically, I want to create an API that allows users to send input data (e.g., JSON with column values), and under the hood, it runs my dbt model using that input and returns the transformed output as defined by the model.\n\nFor example, suppose I have a dbt model called `my_model` (in reality the model is a lot more complex):\n\n    select \n        {{ macro_1("col_1") }} as out_col_1,\n        {{ macro_2("col_1", "col_2") }} as out_col_2\n    from \n        {{ ref(\'input_model_or_data\') }}\n    \n\nNormally, `ref(\'input_model_or_data\')` would resolve to another dbt model, but I’ve seen in dbt unit tests that you can inject synthetic data into that `ref()`, like this:\n\n    - name: test_my_model\n      model: my_model\n      given:\n        - input: ref(\'input_model_or_data\')\n          rows:\n            - {col_1: \'val_1\', col_2: 1}\n      expect:\n        rows:\n          - {out_col_1: "out_val_1", out_col_2: "out_val_2"}\n    \n\nThis allows the test to override the input source. I’d like to do something similar via an API: the user sends input like `{col_1: \'val_1\', col_2: 1}` to an endpoint, and the API returns the output of the dbt model (e.g., `{out_col_1: "out_val_1", out_col_2: "out_val_2"}`), having used that input as the data behind `ref(\'input_model_or_data\')`.\n\nWhat’s the recommended way to do something like this?', 'author_fullname': 't2_s9545lpak', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to build an API on top of a dbt model?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr1azg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747736149.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have quite a complex SQL query within DBT which I have been tasked to build an API &#39;on top of&#39;.</p>\n\n<p>More specifically, I want to create an API that allows users to send input data (e.g., JSON with column values), and under the hood, it runs my dbt model using that input and returns the transformed output as defined by the model.</p>\n\n<p>For example, suppose I have a dbt model called <code>my_model</code> (in reality the model is a lot more complex):</p>\n\n<pre><code>select \n    {{ macro_1(&quot;col_1&quot;) }} as out_col_1,\n    {{ macro_2(&quot;col_1&quot;, &quot;col_2&quot;) }} as out_col_2\nfrom \n    {{ ref(&#39;input_model_or_data&#39;) }}\n</code></pre>\n\n<p>Normally, <code>ref(&#39;input_model_or_data&#39;)</code> would resolve to another dbt model, but I’ve seen in dbt unit tests that you can inject synthetic data into that <code>ref()</code>, like this:</p>\n\n<pre><code>- name: test_my_model\n  model: my_model\n  given:\n    - input: ref(&#39;input_model_or_data&#39;)\n      rows:\n        - {col_1: &#39;val_1&#39;, col_2: 1}\n  expect:\n    rows:\n      - {out_col_1: &quot;out_val_1&quot;, out_col_2: &quot;out_val_2&quot;}\n</code></pre>\n\n<p>This allows the test to override the input source. I’d like to do something similar via an API: the user sends input like <code>{col_1: &#39;val_1&#39;, col_2: 1}</code> to an endpoint, and the API returns the output of the dbt model (e.g., <code>{out_col_1: &quot;out_val_1&quot;, out_col_2: &quot;out_val_2&quot;}</code>), having used that input as the data behind <code>ref(&#39;input_model_or_data&#39;)</code>.</p>\n\n<p>What’s the recommended way to do something like this?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kr1azg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bebmfec'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr1azg/how_to_build_an_api_on_top_of_a_dbt_model/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr1azg/how_to_build_an_api_on_top_of_a_dbt_model/', 'subreddit_subscribers': 328156, 'created_utc': 1747736149.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.979+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "dbt seems to be getting locked more and more into Visual Studio Code, there new addon means the best developer experience will probably be VSCode followed by their dbt Cloud offering.\n\n  \nI don't really mind this but as a hobbyist tinkerer, it feels a bit closed for my liking.\n\nIs there any community effort to build out an LSP or other integrations for the vim users, or other editors I could explore?\n\nChatGPT seems to suggest FiveTran had an attempt at it but it seems like it was discontinued.", 'author_fullname': 't2_41nvl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Does dbt have a language server?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krfoh4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747773839.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>dbt seems to be getting locked more and more into Visual Studio Code, there new addon means the best developer experience will probably be VSCode followed by their dbt Cloud offering.</p>\n\n<p>I don&#39;t really mind this but as a hobbyist tinkerer, it feels a bit closed for my liking.</p>\n\n<p>Is there any community effort to build out an LSP or other integrations for the vim users, or other editors I could explore?</p>\n\n<p>ChatGPT seems to suggest FiveTran had an attempt at it but it seems like it was discontinued.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1krfoh4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='wallyflops'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krfoh4/does_dbt_have_a_language_server/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krfoh4/does_dbt_have_a_language_server/', 'subreddit_subscribers': 328156, 'created_utc': 1747773839.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.979+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_gpjiq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A Distributed System from scratch, with Scala 3 - Part 3: Job submission, worker scaling, and leader election & consensus with Raft', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 109, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr4wtk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': 'transparent', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '287cf772-ac9d-11eb-aa84-0ead36cb44af', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/oY2_-j5lHsnO9A1VCk7dmWapoBuAGcV7Ce-9Pgf9vrk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747747804.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'chollinger.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://chollinger.com/blog/2025/05/a-distributed-system-from-scratch-with-scala-3-part-3-job-submission-worker-scaling-and-leader-election-consensus-with-raft/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/vk98e2MDAq2kOOZaZnOpT-Y2sPS5ASJccPEgOItTWtI.jpg?auto=webp&s=6be1bc15fde848a4b036a9140b44bf14fc27169c', 'width': 614, 'height': 482}, 'resolutions': [{'url': 'https://external-preview.redd.it/vk98e2MDAq2kOOZaZnOpT-Y2sPS5ASJccPEgOItTWtI.jpg?width=108&crop=smart&auto=webp&s=077f260146fd63660c63ba02276f098b36e58c5d', 'width': 108, 'height': 84}, {'url': 'https://external-preview.redd.it/vk98e2MDAq2kOOZaZnOpT-Y2sPS5ASJccPEgOItTWtI.jpg?width=216&crop=smart&auto=webp&s=29e0678f887e1c3e3987696d067e75cbc8aa7ee1', 'width': 216, 'height': 169}, {'url': 'https://external-preview.redd.it/vk98e2MDAq2kOOZaZnOpT-Y2sPS5ASJccPEgOItTWtI.jpg?width=320&crop=smart&auto=webp&s=e9c7a27a8cf6a0e98e99ad937efd59c6bb4ee328', 'width': 320, 'height': 251}], 'variants': {}, 'id': 'jVuA7Ks2T4tpUJhTwWXGrCUoNtX2ulS-aMcn13Cf5Dw'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Staff Software & Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1kr4wtk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='otter-in-a-suit'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1kr4wtk/a_distributed_system_from_scratch_with_scala_3/', 'stickied': False, 'url': 'https://chollinger.com/blog/2025/05/a-distributed-system-from-scratch-with-scala-3-part-3-job-submission-worker-scaling-and-leader-election-consensus-with-raft/', 'subreddit_subscribers': 328156, 'created_utc': 1747747804.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.979+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_baajg5kk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Reverse Sampling: Rethinking How We Test Data Pipelines', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr3wb2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/WZmu1gy0flScm_a5ZXQW2PTSfjVDtME3eRzHOqBJ6CY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747744932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'moderndata101.substack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://moderndata101.substack.com/p/reverse-sampling-rethinking-data-pipelines', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?auto=webp&s=113711c06afabf20326d968fe6c1b476b20b22d2', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=108&crop=smart&auto=webp&s=9e61d98cefa3d2bacc1fb5f4e8a72be28a57fca0', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=216&crop=smart&auto=webp&s=321fef0b1abf2e4d4888792b256c14fe037c3e0f', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=320&crop=smart&auto=webp&s=1b8ed777fea10b52a7ad567f6998cceab9184ec1', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=640&crop=smart&auto=webp&s=2c2261c00ef305be18ea1b47f3cb7661ac67f50c', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=960&crop=smart&auto=webp&s=061dc887dbfe06d5d7bae305ea94332cda15e0c8', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/BANU6CqOrBw29dZ71LAaLIVDQH7xwPm9dJWuEzdyeaE.jpg?width=1080&crop=smart&auto=webp&s=9452fa1053bad4523fbb6ff19fc9c43c71d2729f', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'o0dr7mjPCepD4bktWPhuZXTD04TMFd9uc7oN9pSqgpM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1kr3wb2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='growth_man'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr3wb2/reverse_sampling_rethinking_how_we_test_data/', 'stickied': False, 'url': 'https://moderndata101.substack.com/p/reverse-sampling-rethinking-data-pipelines', 'subreddit_subscribers': 328156, 'created_utc': 1747744932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.979+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I\'m building an entirely on-premise conversational AI agent that lets users query SQL, NoSQL (MongoDB), and vector (Qdrant) stores using natural language. We rely on an embedded schema registry to:\n\n1. Drive natural language to query generation across heterogeneous stores\n2. Enable multi-database joins in a single conversation\n3. Handle schema evolution without downtime\n\n**Key questions:**\n\n* How do you version and enforce compatibility checks when your registry is hosted in-house (e.g., in SQLite) and needs to serve sub-100 ms lookups? For smaller databases, it\'s not a problem, but for multiple databases, each with millions of rows, how do you make this validation quick?\n* What patterns keep adapters "pluggable" and synchronized as source schemas evolve (think Protobuf → JSON → Avro migrations)?\n* How have you handled backward compatibility when deprecating fields while still supporting historical natural language sessions?\n\nI\'d especially appreciate insights from those who have built custom registries/adapters in regulated environments where cloud services aren\'t an option.\n\nThanks in advance for any pointers or war stories!', 'author_fullname': 't2_hl3hunh34', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Designing Robust Schema Registry Systems for On-Premise Data Infrastructure', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqyceq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747723531.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m building an entirely on-premise conversational AI agent that lets users query SQL, NoSQL (MongoDB), and vector (Qdrant) stores using natural language. We rely on an embedded schema registry to:</p>\n\n<ol>\n<li>Drive natural language to query generation across heterogeneous stores</li>\n<li>Enable multi-database joins in a single conversation</li>\n<li>Handle schema evolution without downtime</li>\n</ol>\n\n<p><strong>Key questions:</strong></p>\n\n<ul>\n<li>How do you version and enforce compatibility checks when your registry is hosted in-house (e.g., in SQLite) and needs to serve sub-100 ms lookups? For smaller databases, it&#39;s not a problem, but for multiple databases, each with millions of rows, how do you make this validation quick?</li>\n<li>What patterns keep adapters &quot;pluggable&quot; and synchronized as source schemas evolve (think Protobuf → JSON → Avro migrations)?</li>\n<li>How have you handled backward compatibility when deprecating fields while still supporting historical natural language sessions?</li>\n</ul>\n\n<p>I&#39;d especially appreciate insights from those who have built custom registries/adapters in regulated environments where cloud services aren&#39;t an option.</p>\n\n<p>Thanks in advance for any pointers or war stories!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kqyceq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ScienceInformal3001'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqyceq/designing_robust_schema_registry_systems_for/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kqyceq/designing_robust_schema_registry_systems_for/', 'subreddit_subscribers': 328156, 'created_utc': 1747723531.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.980+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\n\nI\'m working on a\xa0**product classifier**\xa0for ecommerce listings, and I\'m looking for advice on the best way to\xa0**extract specific attributes/features**\xa0from product titles, such as the\xa0**number of doors in a wardrobe**.\n\nFor example, I have titles like:\n\n* 🟢\xa0*"BRAND X Kayden Engineered Wood 3 Door Wardrobe for Clothes, Cupboard Wooden Almirah for Bedroom, Multi Utility Wardrobe with Hanger Rod Lock and Handles,1 Year Warranty, Columbian Walnut Finis*h"\n* 🔵\xa0*"BRAND X Kayden Engineered Wood 5 Door Wardrobe for Clothes, Cupboard Wooden Almirah for Bedroom, Multi Utility Wardrobe with Hanger Rod Lock and Handles,1 Year Warranty, Columbian Walnut Finis*h"\n\nI need to design a logic or model that can correctly\xa0**differentiate between these products**\xa0based on the number of doors (in this case,\xa0**3 Door**\xa0vs\xa0**5 Door**).\n\nI\'m considering approaches like:\n\n* Regex-based rule extraction (e.g., extracting\xa0`(\\d+)\\s+door`)\n* Using a tokenizer + keyword attention model\n* Fine-tuning a small transformer model to extract structured attributes\n* Dependency parsing to associate numerals with the right product feature\n\nHas anyone tackled a similar problem? I\'d love to hear:\n\n* What worked for you?\n* Would you recommend a rule-based, ML-based, or hybrid approach?\n* How do you handle generalization to other attributes like material, color, or dimensions?\n\nThanks in advance! 🙏', 'author_fullname': 't2_aifr3zo7e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Attribute/features extraction logic for ecommerce product titles', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqwy6k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747717832.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m working on a\xa0<strong>product classifier</strong>\xa0for ecommerce listings, and I&#39;m looking for advice on the best way to\xa0<strong>extract specific attributes/features</strong>\xa0from product titles, such as the\xa0<strong>number of doors in a wardrobe</strong>.</p>\n\n<p>For example, I have titles like:</p>\n\n<ul>\n<li>🟢\xa0<em>&quot;BRAND X Kayden Engineered Wood 3 Door Wardrobe for Clothes, Cupboard Wooden Almirah for Bedroom, Multi Utility Wardrobe with Hanger Rod Lock and Handles,1 Year Warranty, Columbian Walnut Finis</em>h&quot;</li>\n<li>🔵\xa0<em>&quot;BRAND X Kayden Engineered Wood 5 Door Wardrobe for Clothes, Cupboard Wooden Almirah for Bedroom, Multi Utility Wardrobe with Hanger Rod Lock and Handles,1 Year Warranty, Columbian Walnut Finis</em>h&quot;</li>\n</ul>\n\n<p>I need to design a logic or model that can correctly\xa0<strong>differentiate between these products</strong>\xa0based on the number of doors (in this case,\xa0<strong>3 Door</strong>\xa0vs\xa0<strong>5 Door</strong>).</p>\n\n<p>I&#39;m considering approaches like:</p>\n\n<ul>\n<li>Regex-based rule extraction (e.g., extracting\xa0<code>(\\d+)\\s+door</code>)</li>\n<li>Using a tokenizer + keyword attention model</li>\n<li>Fine-tuning a small transformer model to extract structured attributes</li>\n<li>Dependency parsing to associate numerals with the right product feature</li>\n</ul>\n\n<p>Has anyone tackled a similar problem? I&#39;d love to hear:</p>\n\n<ul>\n<li>What worked for you?</li>\n<li>Would you recommend a rule-based, ML-based, or hybrid approach?</li>\n<li>How do you handle generalization to other attributes like material, color, or dimensions?</li>\n</ul>\n\n<p>Thanks in advance! 🙏</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1kqwy6k', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Problemsolver_11'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqwy6k/attributefeatures_extraction_logic_for_ecommerce/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kqwy6k/attributefeatures_extraction_logic_for_ecommerce/', 'subreddit_subscribers': 328156, 'created_utc': 1747717832.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.980+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_8isdv2tf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Mastering Databricks Real-Time Analytics with Spark Structured Streaming', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqrrs2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/hpjsWfPjJyI?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Mastering Databricks Real-Time Analytics with Spark Structured Streaming"></iframe>', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Mastering Databricks Real-Time Analytics with Spark Structured Streaming', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/hpjsWfPjJyI?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Mastering Databricks Real-Time Analytics with Spark Structured Streaming"></iframe>', 'author_name': 'Fikrat Azizov', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/hpjsWfPjJyI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@fazizov'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/hpjsWfPjJyI?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Mastering Databricks Real-Time Analytics with Spark Structured Streaming"></iframe>', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1kqrrs2', 'height': 200}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/BulExP8Fw6-x5MQFgnYdvJGcpHiaZn5Wadxj-NPaUX8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747701108.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://youtu.be/hpjsWfPjJyI', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/R42jLT2ps1OkRzL2HfIyGd_eTmLHXF77CUorRiW1bc8.jpg?auto=webp&s=b512ff2d691326a2153a22c7f608acea0960eb9e', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/R42jLT2ps1OkRzL2HfIyGd_eTmLHXF77CUorRiW1bc8.jpg?width=108&crop=smart&auto=webp&s=38f3056a73d290f43f79274167abc50ec366fb78', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/R42jLT2ps1OkRzL2HfIyGd_eTmLHXF77CUorRiW1bc8.jpg?width=216&crop=smart&auto=webp&s=4fb893b9599992fbc3ad10bbefeef5bdca7b6c41', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/R42jLT2ps1OkRzL2HfIyGd_eTmLHXF77CUorRiW1bc8.jpg?width=320&crop=smart&auto=webp&s=957681deedb85bab2f2684ac56f08df674a730cb', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'aiK1NTwqKn_nz6_ZCNaXBVU9jRqBVn082bcILFsnh9w'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1kqrrs2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Nice_Substance_6594'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqrrs2/mastering_databricks_realtime_analytics_with/', 'stickied': False, 'url': 'https://youtu.be/hpjsWfPjJyI', 'subreddit_subscribers': 328156, 'created_utc': 1747701108.0, 'num_crossposts': 0, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Mastering Databricks Real-Time Analytics with Spark Structured Streaming', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/hpjsWfPjJyI?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Mastering Databricks Real-Time Analytics with Spark Structured Streaming"></iframe>', 'author_name': 'Fikrat Azizov', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/hpjsWfPjJyI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@fazizov'}}, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.981+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In short, currently the BI department in my org is managing all of the accesses to data & tools. \n\nAccording to them, only they should have an access to the data warehouse, everyone else should only use Looker and if needed , extract stuff from Looker to excel and manipulate/run calculations.\n\nIn my opinion this is insane as the we have numerous people on high payrolls within marketing, finance departments with analytical background and skills with SQL/Python. \n\nIs this usual? This eliminates any autonomy and slows everything down substantially, as any new development has to go through sprints and prioritization.', 'author_fullname': 't2_15c36m48y6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How does your organization manage the accesses to the data?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1krj10a', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747782537.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In short, currently the BI department in my org is managing all of the accesses to data &amp; tools. </p>\n\n<p>According to them, only they should have an access to the data warehouse, everyone else should only use Looker and if needed , extract stuff from Looker to excel and manipulate/run calculations.</p>\n\n<p>In my opinion this is insane as the we have numerous people on high payrolls within marketing, finance departments with analytical background and skills with SQL/Python. </p>\n\n<p>Is this usual? This eliminates any autonomy and slows everything down substantially, as any new development has to go through sprints and prioritization.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1krj10a', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='TikraiNeMentas'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krj10a/how_does_your_organization_manage_the_accesses_to/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krj10a/how_does_your_organization_manage_the_accesses_to/', 'subreddit_subscribers': 328156, 'created_utc': 1747782537.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.981+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi there!\n\nWhile I originally have an Chem Eng background, I mostly worked in operations & marketing past few years & been exploring data analytics & science past 2 years including Python (pandas, numpy, sklearn, etc.), SQL, etc.\n\nI am really passionate about data as well as analytics so am keen to dive deeper into each, in terms of data engineering & automation as well as advanced AI/ML engineering. Does it make sense to do courses in both? There seems to be some commonalities especially with using Python. Also it probably might be helpful to have a good understanding of both while working deeply in one. For context, most of my knoweldge has only been academic with some jupiter projects & haven't really explored the world of databases, cloud, github, etc.\n\nThere are these following programs on Coursera that I'm looking into as a start (feel free to just advise on DE given the subreddit):\n\nData Engineering:\n\n[https://www.coursera.org/professional-certificates/ibm-data-engineer](https://www.coursera.org/professional-certificates/ibm-data-engineer)\n\n[https://www.coursera.org/professional-certificates/data-engineering](https://www.coursera.org/professional-certificates/data-engineering)\n\nAI/ML Eng:\n\n[https://www.coursera.org/professional-certificates/ai-engineer](https://www.coursera.org/professional-certificates/ai-engineer)\n\n[https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai](https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai)\n\n[https://www.coursera.org/specializations/ibm-ai-workflow](https://www.coursera.org/specializations/ibm-ai-workflow)\n\n(& some standalone RAG/Langchain/ML projects)  \nAutomation:  \n[https://www.coursera.org/professional-certificates/google-it-automation](https://www.coursera.org/professional-certificates/google-it-automation)\n\nWould really appreciate any guidance/suggestions with above!\n\n(I'm well aware even all of these might not be enough to get me even an entry job in either areas but I think it's a good start, especially since I'm currently semi-unemployed with lots of free time & a paid Coursera subscription that I should take advantage of).", 'author_fullname': 't2_761vrk2j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data enthusiast looking to dive more into data & AI/ML engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr988c', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1747758998.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747758465.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi there!</p>\n\n<p>While I originally have an Chem Eng background, I mostly worked in operations &amp; marketing past few years &amp; been exploring data analytics &amp; science past 2 years including Python (pandas, numpy, sklearn, etc.), SQL, etc.</p>\n\n<p>I am really passionate about data as well as analytics so am keen to dive deeper into each, in terms of data engineering &amp; automation as well as advanced AI/ML engineering. Does it make sense to do courses in both? There seems to be some commonalities especially with using Python. Also it probably might be helpful to have a good understanding of both while working deeply in one. For context, most of my knoweldge has only been academic with some jupiter projects &amp; haven&#39;t really explored the world of databases, cloud, github, etc.</p>\n\n<p>There are these following programs on Coursera that I&#39;m looking into as a start (feel free to just advise on DE given the subreddit):</p>\n\n<p>Data Engineering:</p>\n\n<p><a href="https://www.coursera.org/professional-certificates/ibm-data-engineer">https://www.coursera.org/professional-certificates/ibm-data-engineer</a></p>\n\n<p><a href="https://www.coursera.org/professional-certificates/data-engineering">https://www.coursera.org/professional-certificates/data-engineering</a></p>\n\n<p>AI/ML Eng:</p>\n\n<p><a href="https://www.coursera.org/professional-certificates/ai-engineer">https://www.coursera.org/professional-certificates/ai-engineer</a></p>\n\n<p><a href="https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai">https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai</a></p>\n\n<p><a href="https://www.coursera.org/specializations/ibm-ai-workflow">https://www.coursera.org/specializations/ibm-ai-workflow</a></p>\n\n<p>(&amp; some standalone RAG/Langchain/ML projects)<br/>\nAutomation:<br/>\n<a href="https://www.coursera.org/professional-certificates/google-it-automation">https://www.coursera.org/professional-certificates/google-it-automation</a></p>\n\n<p>Would really appreciate any guidance/suggestions with above!</p>\n\n<p>(I&#39;m well aware even all of these might not be enough to get me even an entry job in either areas but I think it&#39;s a good start, especially since I&#39;m currently semi-unemployed with lots of free time &amp; a paid Coursera subscription that I should take advantage of).</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?auto=webp&s=dec206b184a92cb8e4e5794baed511b679b76081', 'width': 1772, 'height': 928}, 'resolutions': [{'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=108&crop=smart&auto=webp&s=3a42eaee84764c493db4538ac2650eeafe673ee8', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=216&crop=smart&auto=webp&s=2b70b31b1c3fa1053c83dc82bbff1e442413830a', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=320&crop=smart&auto=webp&s=7b2c273558fa5f7a512d444ae01b3b34a5d69081', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=640&crop=smart&auto=webp&s=050db9c59146d7dcaffa69144fc468c8012973fa', 'width': 640, 'height': 335}, {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=960&crop=smart&auto=webp&s=d9851ddadb32f38810b7d0c543f590a933464700', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/L9DS2eknGWKm5d05i7VCl-ZqYk8fObpPwkZ0foCjmQ8.jpg?width=1080&crop=smart&auto=webp&s=0d1ca6d58eb56e4836f925d1ec890da955aa716d', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'zClXlRxo1471XrjC4DaSZJPDNryJmUU5tZrdtt4o510'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kr988c', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RazzmatazzBitter4383'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr988c/data_enthusiast_looking_to_dive_more_into_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr988c/data_enthusiast_looking_to_dive_more_into_data/', 'subreddit_subscribers': 328156, 'created_utc': 1747758465.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.981+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I work at a company where we have some web scrapers made using a proprietary technology that we’re trying to get rid of. \n\nWe have permission to scrape the websites that we are scraping, if that impacts anything.\n\nI was wondering if Dagster is the appropriate tool to orchestrate selenium based web scraping and have it run on AWS using docker and EC2 most likely.\n\nAny insights are much appreciated!', 'author_fullname': 't2_ahp39wfd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Does it make sense to use Dagster for web scraping', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr4nw5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747747122.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I work at a company where we have some web scrapers made using a proprietary technology that we’re trying to get rid of. </p>\n\n<p>We have permission to scrape the websites that we are scraping, if that impacts anything.</p>\n\n<p>I was wondering if Dagster is the appropriate tool to orchestrate selenium based web scraping and have it run on AWS using docker and EC2 most likely.</p>\n\n<p>Any insights are much appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kr4nw5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Jazzlike_Middle2757'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr4nw5/does_it_make_sense_to_use_dagster_for_web_scraping/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr4nw5/does_it_make_sense_to_use_dagster_for_web_scraping/', 'subreddit_subscribers': 328156, 'created_utc': 1747747122.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.982+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "First, I apologize if I'm posting this in the wrong place and if my question is dumb.\n\n**Business Problem**\n\nWe are a very small independent book publisher. Today, sales from various distribution channels come to us as spreadsheets. Each distributor's sheet is different. We need to get the information into our own homegrown sales and royalty system. \n\nWe have created a common import sheet, and today, we manually copy and paste and map data from the various sheets into our common import format. In many cases, we have to add data, such as currency codes, conversion rates, and transform the values into our own currency.\n\nI've been looking for tools for the Mac, where I can define each sheet that comes in and where that data goes in a common format. The only thing we have today is a document that tells the person moving the data what goes where, and in some cases of distributors, that field should be null in the common input format.\n\nI'd like to automate this data transfer process, or is affordable software to automate the transfer and mapping a pipe dream?\n\n", 'author_fullname': 't2_8m5dein9c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tool to Map Data From One Excel Sheet to Another - Goal Data Import', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqs7sf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747702466.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>First, I apologize if I&#39;m posting this in the wrong place and if my question is dumb.</p>\n\n<p><strong>Business Problem</strong></p>\n\n<p>We are a very small independent book publisher. Today, sales from various distribution channels come to us as spreadsheets. Each distributor&#39;s sheet is different. We need to get the information into our own homegrown sales and royalty system. </p>\n\n<p>We have created a common import sheet, and today, we manually copy and paste and map data from the various sheets into our common import format. In many cases, we have to add data, such as currency codes, conversion rates, and transform the values into our own currency.</p>\n\n<p>I&#39;ve been looking for tools for the Mac, where I can define each sheet that comes in and where that data goes in a common format. The only thing we have today is a document that tells the person moving the data what goes where, and in some cases of distributors, that field should be null in the common input format.</p>\n\n<p>I&#39;d like to automate this data transfer process, or is affordable software to automate the transfer and mapping a pipe dream?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kqs7sf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Rattling_Good_Yarns'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqs7sf/tool_to_map_data_from_one_excel_sheet_to_another/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kqs7sf/tool_to_map_data_from_one_excel_sheet_to_another/', 'subreddit_subscribers': 328156, 'created_utc': 1747702466.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.982+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello,\n\nI'm not even sure if this post should be here but since my internship role is data engineering, i am asking because i'm sure a lot of experienced data engineers who have had problems like this will read this.\n\n At our utilities company, we manage gas and heating meters and face data quality challenges with both manual and IoT-based meter readings. Manual readings, entered on-site by technicians via a CMMS tool, and IoT-based automatic readings, collected by connected meters and sent directly to BigQuery via ingestion pipelines, currently lack validation. The IoT pipeline is particularly problematic, inserting large volumes of unverified data into our analytics database without checks for anomalies, inconsistencies, or hardware malfunctions. To address this, we aim to design a functional validation framework before selecting technical tools.\n\nKey considerations include defining validation rules, handling invalid or suspect data and applying confidence scoring to readings, comparing IoT and manual readings to reconcile discrepancies. We seek functional ideas, best practices, and examples of validation frameworks, particularly for IoT, utilities, or time-series data, focusing on documentation approaches, validation strategies, and operational processes to guide our implementation.\n\nThanks to everyone who takes time to answer, we don't even know how to start setting up our data pipeline since we can't define anomaly standards yet and what actions to do in case of anomaly detection. ", 'author_fullname': 't2_4xsxew8v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to define a validation framework for IoT and manual meter readings before analytics?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krforx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747773861.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>I&#39;m not even sure if this post should be here but since my internship role is data engineering, i am asking because i&#39;m sure a lot of experienced data engineers who have had problems like this will read this.</p>\n\n<p>At our utilities company, we manage gas and heating meters and face data quality challenges with both manual and IoT-based meter readings. Manual readings, entered on-site by technicians via a CMMS tool, and IoT-based automatic readings, collected by connected meters and sent directly to BigQuery via ingestion pipelines, currently lack validation. The IoT pipeline is particularly problematic, inserting large volumes of unverified data into our analytics database without checks for anomalies, inconsistencies, or hardware malfunctions. To address this, we aim to design a functional validation framework before selecting technical tools.</p>\n\n<p>Key considerations include defining validation rules, handling invalid or suspect data and applying confidence scoring to readings, comparing IoT and manual readings to reconcile discrepancies. We seek functional ideas, best practices, and examples of validation frameworks, particularly for IoT, utilities, or time-series data, focusing on documentation approaches, validation strategies, and operational processes to guide our implementation.</p>\n\n<p>Thanks to everyone who takes time to answer, we don&#39;t even know how to start setting up our data pipeline since we can&#39;t define anomaly standards yet and what actions to do in case of anomaly detection. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1krforx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Way-8559'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krforx/how_to_define_a_validation_framework_for_iot_and/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krforx/how_to_define_a_validation_framework_for_iot_and/', 'subreddit_subscribers': 328156, 'created_utc': 1747773861.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.982+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "KumoRFM handles instant predictive tasks over enterprise/structured data.\n\nThey’ve detailed how it works: the model turns relational databases into graphs, uses in-context examples (pulled straight from the data), and makes predictions without task-specific training.\n\nIt can predict things like user churn, product demand, fraud, or what item a user might click next, without writing custom models.\n\nThere's a technical blog and a whitepaper\n\n[https://kumo.ai/company/news/kumo-relational-foundation-model/](https://kumo.ai/company/news/kumo-relational-foundation-model/)", 'author_fullname': 't2_1me5rjftax', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'General-purpose model for making instant predictions over relational data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krc33s', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747765144.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>KumoRFM handles instant predictive tasks over enterprise/structured data.</p>\n\n<p>They’ve detailed how it works: the model turns relational databases into graphs, uses in-context examples (pulled straight from the data), and makes predictions without task-specific training.</p>\n\n<p>It can predict things like user churn, product demand, fraud, or what item a user might click next, without writing custom models.</p>\n\n<p>There&#39;s a technical blog and a whitepaper</p>\n\n<p><a href="https://kumo.ai/company/news/kumo-relational-foundation-model/">https://kumo.ai/company/news/kumo-relational-foundation-model/</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?auto=webp&s=7c9caeab8b43db70870e283fa150e2259bf024f4', 'width': 960, 'height': 576}, 'resolutions': [{'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?width=108&crop=smart&auto=webp&s=e544007b2abe2d10e0e27c74a0fd557f4f54b3da', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?width=216&crop=smart&auto=webp&s=e12ef9697cb294c82c3a52fd796852f5707fcaea', 'width': 216, 'height': 129}, {'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?width=320&crop=smart&auto=webp&s=5c0fb1fdd4ffb203a603f63119c26c48a81f3dd5', 'width': 320, 'height': 192}, {'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?width=640&crop=smart&auto=webp&s=a5e3bc3c4d2c0ac7826e7e06ac5b12545d080a1e', 'width': 640, 'height': 384}, {'url': 'https://external-preview.redd.it/lHFnpq-613fsLiNxx6Rz8QGR9psFFdlVthqPdrXRgFo.jpg?width=960&crop=smart&auto=webp&s=8e2164a054739c2784ce453a24e1ffe67640a342', 'width': 960, 'height': 576}], 'variants': {}, 'id': 'DeolSba2SErYCq8UX3GV5Kw2-MEECtVi8-W19MwBLiY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1krc33s', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Outhere9977'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krc33s/generalpurpose_model_for_making_instant/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krc33s/generalpurpose_model_for_making_instant/', 'subreddit_subscribers': 328156, 'created_utc': 1747765144.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.983+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey yall,\n\nI'm a one man show at my company and I've been tasked with helping pipe data from our Snowflake warehouse into Salesforce. My current tech stack is Fivetran, dbt cloud, and Snowflake and I was hoping there would be some integrations that are affordable amongst these tools to make this happen reliably and affordably without having to build out a bunch of custom infra that I'd have to maintain. The options I've seen (specifically salesforce connect) are not affordable.\n\nThanks!", 'author_fullname': 't2_dkfbs', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Easiest/most affordable way to move data from Snowflake to Salesforce.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krbz8o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1747765403.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747764888.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey yall,</p>\n\n<p>I&#39;m a one man show at my company and I&#39;ve been tasked with helping pipe data from our Snowflake warehouse into Salesforce. My current tech stack is Fivetran, dbt cloud, and Snowflake and I was hoping there would be some integrations that are affordable amongst these tools to make this happen reliably and affordably without having to build out a bunch of custom infra that I&#39;d have to maintain. The options I&#39;ve seen (specifically salesforce connect) are not affordable.</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1krbz8o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='biga410'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krbz8o/easiestmost_affordable_way_to_move_data_from/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krbz8o/easiestmost_affordable_way_to_move_data_from/', 'subreddit_subscribers': 328156, 'created_utc': 1747764888.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.983+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey everyone – I'm an independent privacy researcher exploring how orgs like yours discover and classify personal data (PII) across systems, especially under GDPR, or CCPA.\n\nI’ve created a short, focused 6–8 minute survey (completely anonymous) to learn what’s working, what’s frustrating, and what tools actually deliver value.\n\nYour input helps identify real pain points the privacy/security community faces today. \n\nThanks for helping out — happy to share results with the community if folks are interested.", 'author_fullname': 't2_v1bfe1aq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do privacy team really need from data discovery tools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqzlwu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/FU79O9VTPgTgyrVe5I-6ZxGDtFt799MkfVJH-BbzrEs.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747728950.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'surveymonkey.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone – I&#39;m an independent privacy researcher exploring how orgs like yours discover and classify personal data (PII) across systems, especially under GDPR, or CCPA.</p>\n\n<p>I’ve created a short, focused 6–8 minute survey (completely anonymous) to learn what’s working, what’s frustrating, and what tools actually deliver value.</p>\n\n<p>Your input helps identify real pain points the privacy/security community faces today. </p>\n\n<p>Thanks for helping out — happy to share results with the community if folks are interested.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.surveymonkey.com/r/JX7CTY3', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?auto=webp&s=c7115a61a8436f2fc01d83a8cb80f902f69a8e47', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=108&crop=smart&auto=webp&s=40d33ce7153cabdbaa6afe2afac1c2b2fd61bbb3', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=216&crop=smart&auto=webp&s=9f687c82f6e17f2618c5009b9deabe0db4d6b0e7', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=320&crop=smart&auto=webp&s=f6a49b1e022e3497710d7ba41dbe56123cb4c524', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=640&crop=smart&auto=webp&s=b17a5ebdb05ab063a477bd8c736afa65873554ee', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=960&crop=smart&auto=webp&s=3525e3c3c1b8437a68eea54d9d3c009524638a8d', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/U62a2QDALc4BILSNAz9PIt1H04k8jwUffAGS8mzm3D4.jpg?width=1080&crop=smart&auto=webp&s=fb4dad22526299584fdf93b597aeda79d8387058', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'Sorct-p_6GU7fDLF7spXPDJwkErqSyPzkSXHXQhJR0c'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kqzlwu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Which_Extension_1852'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqzlwu/what_do_privacy_team_really_need_from_data/', 'stickied': False, 'url': 'https://www.surveymonkey.com/r/JX7CTY3', 'subreddit_subscribers': 328156, 'created_utc': 1747728950.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.984+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey folks! \n\nJust wanted to share something cool from the team at DataGalaxy. They recently dropped a detailed post about how they’re using Change Data Capture (CDC) to completely rethink how data catalogs work. If you're curious about how companies are tackling some modern data challenges, it’s a solid read.\n\n  \n [Revolutionizing Data Catalogs with CDC: The DataGalaxy Journey](https://engineering.datagalaxy.com/revolutionizing-data-catalogs-with-cdc-the-datagalaxy-journey-b2624b4f7c5f)\n\nWould love to hear what you all think!", 'author_fullname': 't2_1gcs8314', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Revolutionizing Data Catalogs with CDC: The DataGalaxy Journey', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kreeme', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747770726.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks! </p>\n\n<p>Just wanted to share something cool from the team at DataGalaxy. They recently dropped a detailed post about how they’re using Change Data Capture (CDC) to completely rethink how data catalogs work. If you&#39;re curious about how companies are tackling some modern data challenges, it’s a solid read.</p>\n\n<p><a href="https://engineering.datagalaxy.com/revolutionizing-data-catalogs-with-cdc-the-datagalaxy-journey-b2624b4f7c5f">Revolutionizing Data Catalogs with CDC: The DataGalaxy Journey</a></p>\n\n<p>Would love to hear what you all think!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?auto=webp&s=57346347b2bd481735e1d6ca633d0f70602d6bb1', 'width': 1200, 'height': 677}, 'resolutions': [{'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=108&crop=smart&auto=webp&s=ad3f8023725a0a7dba5d80abcf894f51b5cf8e89', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=216&crop=smart&auto=webp&s=86668032335c9d6c7879b3ffc0c96b884d779e51', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=320&crop=smart&auto=webp&s=35e2b3c7ab29bd6f3b896ef690306fdeba0e6879', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=640&crop=smart&auto=webp&s=0fc69d254cb153e4d9c25a301fde6f5e20a75e9b', 'width': 640, 'height': 361}, {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=960&crop=smart&auto=webp&s=eff9ff128e58031fdb8ed2c2b1422eb1152c1745', 'width': 960, 'height': 541}, {'url': 'https://external-preview.redd.it/vZWohw_byNq2ZLT-oszuAq2ZzASQQaJGXgSPEBvXzK8.jpg?width=1080&crop=smart&auto=webp&s=3be0bdf2120b0bf0f58787f6ce6cf2da504f83b9', 'width': 1080, 'height': 609}], 'variants': {}, 'id': 'NGvOr3Tb0K0OIVWKLEypTd61PtQSLVHXX9OkCi0-_Ow'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1kreeme', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Byakuyako'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kreeme/revolutionizing_data_catalogs_with_cdc_the/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kreeme/revolutionizing_data_catalogs_with_cdc_the/', 'subreddit_subscribers': 328156, 'created_utc': 1747770726.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.984+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\nI hope you’re doing well. I’m currently learning data engineering and wanted to share what I’ve built so far — I’d really appreciate any advice, feedback, or suggestions on what to learn next!\n\nHere’s what I’ve worked on:\n\n1. Data Warehouse Star Schema Project\n\t•\tFollowed a YouTube playlist to build a basic data warehouse using PostgreSQL\n\t•\tDesigned a star schema with fact and dimension tables (factSales, dimCustomer, dimMovie, etc.)\n\t•\tWrote SQL queries to extract, transform, and load data\n\nGitHub repo:[Data Warehouse Star Schema Project](https://github.com/SwapnilDawar2004/Data-Warehouse-Star-Schema-Project)\n\n2. Wealth Data Modelling Project\n\t•\tSet up a PostgreSQL database to store and manage financial account data\n\t•\tUsed Python, Pandas, and psycopg2 for data cleaning and database interaction\n\t•\tBuilt everything in Jupyter Notebook using a Kaggle dataset\n\nGitHub repo: [Wealth Data Modelling Project](https://github.com/SwapnilDawar2004/Wealth-Data-Modelling-Project)\n\nI’d love to know What should I focus on next to improve my skills? Any tips on what to do better for internships or job opportunities?\n\nThanks in advance for any help', 'author_fullname': 't2_eciwvvur', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learning Data Engineering. Would Love Your Feedback and Advice!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr22qd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1747739564.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747739006.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,\nI hope you’re doing well. I’m currently learning data engineering and wanted to share what I’ve built so far — I’d really appreciate any advice, feedback, or suggestions on what to learn next!</p>\n\n<p>Here’s what I’ve worked on:</p>\n\n<ol>\n<li>Data Warehouse Star Schema Project\n• Followed a YouTube playlist to build a basic data warehouse using PostgreSQL\n• Designed a star schema with fact and dimension tables (factSales, dimCustomer, dimMovie, etc.)\n• Wrote SQL queries to extract, transform, and load data</li>\n</ol>\n\n<p>GitHub repo:<a href="https://github.com/SwapnilDawar2004/Data-Warehouse-Star-Schema-Project">Data Warehouse Star Schema Project</a></p>\n\n<ol>\n<li>Wealth Data Modelling Project\n• Set up a PostgreSQL database to store and manage financial account data\n• Used Python, Pandas, and psycopg2 for data cleaning and database interaction\n• Built everything in Jupyter Notebook using a Kaggle dataset</li>\n</ol>\n\n<p>GitHub repo: <a href="https://github.com/SwapnilDawar2004/Wealth-Data-Modelling-Project">Wealth Data Modelling Project</a></p>\n\n<p>I’d love to know What should I focus on next to improve my skills? Any tips on what to do better for internships or job opportunities?</p>\n\n<p>Thanks in advance for any help</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?auto=webp&s=6c2b69c7f121d0b0dd32c3e9ac4ae5bec44f5cbf', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=108&crop=smart&auto=webp&s=274f127bd58040c79197c263f5ae6093c06c19e6', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=216&crop=smart&auto=webp&s=eb2a664f67fac35cde55b7ed1595294efd1d67d3', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=320&crop=smart&auto=webp&s=773b9b9a0f90eeab495bf78ae62556e96b87c40f', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=640&crop=smart&auto=webp&s=33621686ac921344c8a7e9da1cc356015bfccafb', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=960&crop=smart&auto=webp&s=557f4dd4e85f56db999cb4a72af7c047eac9afc6', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/-k6yGz7R34EKbo3uTWdb4tPA-5xMJfDe0_G0UVljSJc.jpg?width=1080&crop=smart&auto=webp&s=cd550de5dcb88c02cde505288394e55c37e93e58', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'fIvrOWYPNaza-XB0bmLCuEV-My-359Frk56uGpT7Zik'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1kr22qd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='GarageFederal'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr22qd/learning_data_engineering_would_love_your/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr22qd/learning_data_engineering_would_love_your/', 'subreddit_subscribers': 328156, 'created_utc': 1747739006.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.984+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey, At Vitalops we created a new  open source tool that does data transformations with simple natural langauge instructions and LLMs, without worrying about volume of data in context length or insanely high costs. \n\nCurrently we support:\n\n- Map and Filter operations\n- Use your custom LLM class or, Azure, or use Ollama for local LLM inferencing.\n- Dask Dataframes that supports partitioning and parallel processing \n\n\nCheck it out here, hope it's useful for you!\n\n\nhttps://github.com/vitalops/datatune", 'author_fullname': 't2_fgpnuff3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tool to use LLMs for your data engineering workflow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr1pce', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.4, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1747737872.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747737678.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey, At Vitalops we created a new  open source tool that does data transformations with simple natural langauge instructions and LLMs, without worrying about volume of data in context length or insanely high costs. </p>\n\n<p>Currently we support:</p>\n\n<ul>\n<li>Map and Filter operations</li>\n<li>Use your custom LLM class or, Azure, or use Ollama for local LLM inferencing.</li>\n<li>Dask Dataframes that supports partitioning and parallel processing </li>\n</ul>\n\n<p>Check it out here, hope it&#39;s useful for you!</p>\n\n<p><a href="https://github.com/vitalops/datatune">https://github.com/vitalops/datatune</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?auto=webp&s=b43aeb333d707fc93dca49c66408e0ba2d1f288b', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=108&crop=smart&auto=webp&s=158fabad13a140b6466a0cab38caa174d29fabcd', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=216&crop=smart&auto=webp&s=df11147fc7a31896746fdffb9641111c99928664', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=320&crop=smart&auto=webp&s=4f58947ac5cea468fda8bca42172b6203b420328', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=640&crop=smart&auto=webp&s=edd6b31cd5d50de00100e51fb5a5607469795996', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=960&crop=smart&auto=webp&s=9652554a02deec53332e1df30f7ac982e5eaa6ea', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/ifX_VoDiosRMm84S708v72MuS-GrqspruBibLaPLMqA.jpg?width=1080&crop=smart&auto=webp&s=638ebfe8e197cc3b6defe736851767c16e932645', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'ikafDuHHz_ApKBSMcYgUMITNZQGm99IfciuNVZkNbVs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1kr1pce', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='metalvendetta'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr1pce/tool_to_use_llms_for_your_data_engineering/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kr1pce/tool_to_use_llms_for_your_data_engineering/', 'subreddit_subscribers': 328156, 'created_utc': 1747737678.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.985+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\n\nI\'m 31M feeling incredibly lost and could really use some guidance from this community. I quit my receptionist job abroad at the beginning of 2024 to self-study Data Engineering, and it\'s been a rollercoaster ever since.\n\nI enrolled in Dataquest\'s Data Engineering program and, somehow, managed to finish it recently. It took me a full year, with some gaps in between, but I kept pushing through. I\'ve learned Python, SQL, PostgreSQL, CLI, GitHub, basic algorithms, NumPy, Pandas, and even pipeline concepts.\n\nHere\'s the problem:\xa0**I don\'t feel confident enough to say I truly**\xa0***know***\xa0**any of it.**\xa0My main tool for learning and understanding code and concepts was AI. Now, I feel like I can\'t really code anything without it. Whenever I start a new portfolio project, I feel like I can\'t start or code without seeing a reference or using AI.\n\nThis extensive use of AI has gotten me to a level where\xa0**I can\'t start a script or project without a reference**. It\'s like my brain has become reliant on AI to fill in the gaps, and without it, I\'m stuck. I\'d open a new Python file and immediately feel this overwhelming urge to just type "how to build a data pipeline in Python" into AI. It\'s incredibly frustrating because I\xa0*want*\xa0to be able to build things independently, but I feel crippled without that crutch.\n\nThe thought of a technical inter view absolutely terrifies me. I can picture myself being asked to whiteboard a SQL query or code a simple Python function, and my mind just going blank. Even though I\'ve "learned" these things, the active recall and problem-solving under pressure without AI assistance feels like an insurmountable hurdle. I worry that all my self-study has only given me a shallow understanding that won\'t hold up in a real-world scenario. \n\n# The Web Dev Detour & DE Doubt:\n\nThe weirdest part is, while feeling lost in DE, I stumbled into web development. I built two static websites, [**fixmypdf.in**](http://fixmypdf.in) **and** [**freeinvoiceonline.com**](http://freeinvoiceonline.com)  This was purely "vibe coding" by just messing around, learning HTML, CSS, and JavaScript as along the way. I actually felt a genuine sense of accomplishment and independence there since I self launched myself and really saw some people using it. It makes me question if Data Engineering is even the right path for me, or if I just got lucky with the web dev stuff. It\'s confusing to feel confident in one area I just "vibed" into, but completely lost in the field I\'ve been diligently studying for a year.\n\n# The "Knowing" vs. "Doing" Gap:\n\nI am able to understand  data pipeline or an explanation of a complex SQL join, thinking, "Yeah, I get that." But when it comes to actually*doing*\xa0it, building it, or debugging it myself, it\'s a completely different story. There\'s this huge gap between intellectual understanding and practical application, and I feel like the AI has widened that gap. It\'s almost like I\'ve been a passenger in my own learning journey, and now I need to learn how to drive.\n\n\n\nI feel so lost right now. I don\'t know what to do next, \n\n**What should I do next?**\n\n**How can I truly improve myself to break free from this AI dependency?**\n\n**how to even begin looking for a job,** \n\n**how to truly improve myself to break free from this AI dependency.**\n\n**Are the things I learned while building fixmypdf.in and freeinvoiceonline.com really a waste?**\n\n**Did I really waste my past year of dedicated study? Do I still have hopes/chances to get into Data Engineering?**\n\nHas anyone else experienced this over-reliance on AI during their learning journey? What steps did you take to overcome it? Any advice on how to bridge the gap between theoretical knowledge and practical application, especially for someone trying to break into Data Engineering?\n\nAny guidance, no matter how small, I would really appreciate it. Thanks in advance!', 'author_fullname': 't2_svb5zgn69', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm feeling lost in my DE journey, and extensive AI use has left me crippled", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1krbroc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.27, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747764393.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,</p>\n\n<p>I&#39;m 31M feeling incredibly lost and could really use some guidance from this community. I quit my receptionist job abroad at the beginning of 2024 to self-study Data Engineering, and it&#39;s been a rollercoaster ever since.</p>\n\n<p>I enrolled in Dataquest&#39;s Data Engineering program and, somehow, managed to finish it recently. It took me a full year, with some gaps in between, but I kept pushing through. I&#39;ve learned Python, SQL, PostgreSQL, CLI, GitHub, basic algorithms, NumPy, Pandas, and even pipeline concepts.</p>\n\n<p>Here&#39;s the problem:\xa0<strong>I don&#39;t feel confident enough to say I truly</strong>\xa0<strong><em>know</em></strong>\xa0<strong>any of it.</strong>\xa0My main tool for learning and understanding code and concepts was AI. Now, I feel like I can&#39;t really code anything without it. Whenever I start a new portfolio project, I feel like I can&#39;t start or code without seeing a reference or using AI.</p>\n\n<p>This extensive use of AI has gotten me to a level where\xa0<strong>I can&#39;t start a script or project without a reference</strong>. It&#39;s like my brain has become reliant on AI to fill in the gaps, and without it, I&#39;m stuck. I&#39;d open a new Python file and immediately feel this overwhelming urge to just type &quot;how to build a data pipeline in Python&quot; into AI. It&#39;s incredibly frustrating because I\xa0<em>want</em>\xa0to be able to build things independently, but I feel crippled without that crutch.</p>\n\n<p>The thought of a technical inter view absolutely terrifies me. I can picture myself being asked to whiteboard a SQL query or code a simple Python function, and my mind just going blank. Even though I&#39;ve &quot;learned&quot; these things, the active recall and problem-solving under pressure without AI assistance feels like an insurmountable hurdle. I worry that all my self-study has only given me a shallow understanding that won&#39;t hold up in a real-world scenario. </p>\n\n<h1>The Web Dev Detour &amp; DE Doubt:</h1>\n\n<p>The weirdest part is, while feeling lost in DE, I stumbled into web development. I built two static websites, <a href="http://fixmypdf.in"><strong>fixmypdf.in</strong></a> <strong>and</strong> <a href="http://freeinvoiceonline.com"><strong>freeinvoiceonline.com</strong></a>  This was purely &quot;vibe coding&quot; by just messing around, learning HTML, CSS, and JavaScript as along the way. I actually felt a genuine sense of accomplishment and independence there since I self launched myself and really saw some people using it. It makes me question if Data Engineering is even the right path for me, or if I just got lucky with the web dev stuff. It&#39;s confusing to feel confident in one area I just &quot;vibed&quot; into, but completely lost in the field I&#39;ve been diligently studying for a year.</p>\n\n<h1>The &quot;Knowing&quot; vs. &quot;Doing&quot; Gap:</h1>\n\n<p>I am able to understand  data pipeline or an explanation of a complex SQL join, thinking, &quot;Yeah, I get that.&quot; But when it comes to actually<em>doing</em>\xa0it, building it, or debugging it myself, it&#39;s a completely different story. There&#39;s this huge gap between intellectual understanding and practical application, and I feel like the AI has widened that gap. It&#39;s almost like I&#39;ve been a passenger in my own learning journey, and now I need to learn how to drive.</p>\n\n<p>I feel so lost right now. I don&#39;t know what to do next, </p>\n\n<p><strong>What should I do next?</strong></p>\n\n<p><strong>How can I truly improve myself to break free from this AI dependency?</strong></p>\n\n<p><strong>how to even begin looking for a job,</strong> </p>\n\n<p><strong>how to truly improve myself to break free from this AI dependency.</strong></p>\n\n<p><strong>Are the things I learned while building fixmypdf.in and freeinvoiceonline.com really a waste?</strong></p>\n\n<p><strong>Did I really waste my past year of dedicated study? Do I still have hopes/chances to get into Data Engineering?</strong></p>\n\n<p>Has anyone else experienced this over-reliance on AI during their learning journey? What steps did you take to overcome it? Any advice on how to bridge the gap between theoretical knowledge and practical application, especially for someone trying to break into Data Engineering?</p>\n\n<p>Any guidance, no matter how small, I would really appreciate it. Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?auto=webp&s=5678387e70c134cee7d9d1a441b83c1553c71201', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=108&crop=smart&auto=webp&s=c7d6c3735997bb4a3a0191a570567e520d9db293', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=216&crop=smart&auto=webp&s=1709f57b6286d9a40403befc8e790364780bf705', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=320&crop=smart&auto=webp&s=b7281717c623fed413399849141e46e8bb314bc6', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=640&crop=smart&auto=webp&s=f743c0fdb59c1afdbd3bb8bfd6952cc766a4b4b5', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=960&crop=smart&auto=webp&s=17c7a262e041feb2c179219a62f7f3c6fd7a4716', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/WRhvG965kyuzeO30BxneIw--TGL-homGRidAbYpFqD0.jpg?width=1080&crop=smart&auto=webp&s=109ab49471e28efd20215ed82a4e3afd99fe95a7', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'LjTK5GHF6PaPZ3vOT-oAylaS0xw-Hw2dmKurMp56rPU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1krbroc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ramseywinster'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1krbroc/im_feeling_lost_in_my_de_journey_and_extensive_ai/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1krbroc/im_feeling_lost_in_my_de_journey_and_extensive_ai/', 'subreddit_subscribers': 328156, 'created_utc': 1747764393.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.986+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Similar to Test driven development, I think we are already seeing something we can call "outcome driven development". Think apps like Replit, or perhaps even vibe dashboarding  - where the validation part is you looking at the outcome instead of at the code that was generated.\n\nI recently had to do a migration and i did it that way. Our telemetry data that was feeding to the wrong GCP project. The old pipeline was running an old version of dlt (pre v.1) and the accidental move also upgraded dlt to current version which now typed things slightly differently. There were also missing columns, etc.\n\nLong story short, i worked with Claude 3.7 max (lesser models are a waste of time) and Cursor to create a migration script and validate that it would work, without me actually looking at the python code written by llm - I just looked at the generated SQL and test outcomes (but i didn\'t look if the tests were indeed implemented correctly - just looked at where they failed)\n\nI did the whole migration without reading any generated code (and i am not a YOLO crazy person - it was a calculated risk with a possible recovery pathway). let that sink in. Took 2h instead of 2-3d\n\nDo you have any similar experiences?\n\nEdit: please don\'t downvote because you don\'t like it\'s happening, trying to have dialogue', 'author_fullname': 't2_uamr9xer', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Opinion - "grey box engineering" is here, and we\'re "outcome engineers"', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kqzwgx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.42, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1747732198.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1747730245.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Similar to Test driven development, I think we are already seeing something we can call &quot;outcome driven development&quot;. Think apps like Replit, or perhaps even vibe dashboarding  - where the validation part is you looking at the outcome instead of at the code that was generated.</p>\n\n<p>I recently had to do a migration and i did it that way. Our telemetry data that was feeding to the wrong GCP project. The old pipeline was running an old version of dlt (pre v.1) and the accidental move also upgraded dlt to current version which now typed things slightly differently. There were also missing columns, etc.</p>\n\n<p>Long story short, i worked with Claude 3.7 max (lesser models are a waste of time) and Cursor to create a migration script and validate that it would work, without me actually looking at the python code written by llm - I just looked at the generated SQL and test outcomes (but i didn&#39;t look if the tests were indeed implemented correctly - just looked at where they failed)</p>\n\n<p>I did the whole migration without reading any generated code (and i am not a YOLO crazy person - it was a calculated risk with a possible recovery pathway). let that sink in. Took 2h instead of 2-3d</p>\n\n<p>Do you have any similar experiences?</p>\n\n<p>Edit: please don&#39;t downvote because you don&#39;t like it&#39;s happening, trying to have dialogue</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1kqzwgx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thinker_Assignment'), 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kqzwgx/opinion_grey_box_engineering_is_here_and_were/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1kqzwgx/opinion_grey_box_engineering_is_here_and_were/', 'subreddit_subscribers': 328156, 'created_utc': 1747730245.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.986+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7cbd6ca0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_1amk2offhs', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What?! An Iceberg Catalog that works?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 82, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1kr6jbw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.38, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/eAsf-HudGyvOZPAmUt4Xx6H5QXN0_RorDegIGN35ST8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1747751995.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'dataengineeringcentral.substack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://dataengineeringcentral.substack.com/p/what-an-iceberg-catalog-that-works', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?auto=webp&s=b88fae91ca682090c5017971517fd7f5e9a17e64', 'width': 1024, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?width=108&crop=smart&auto=webp&s=345856be8e2d603553b02f8828430163c1be61b6', 'width': 108, 'height': 63}, {'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?width=216&crop=smart&auto=webp&s=40e876907951551e2cb24df9da79e32310d8c9fa', 'width': 216, 'height': 126}, {'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?width=320&crop=smart&auto=webp&s=d7ce93450868391567e4155d031efdb33b78e06a', 'width': 320, 'height': 187}, {'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?width=640&crop=smart&auto=webp&s=679836a414bc93094c1bd200f740455fc98ed91f', 'width': 640, 'height': 375}, {'url': 'https://external-preview.redd.it/SVECn92nKsvGRelPpWtpwpWYc17xMZbngNlbO7YMKzc.jpg?width=960&crop=smart&auto=webp&s=90fd071d2fa3aac2ffd82e3807ff4359eca0ddc9', 'width': 960, 'height': 562}], 'variants': {}, 'id': '3KBp3cjyAVAv5kCPNCkplJlEXUcmw8kOLeis-3hNyjg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1kr6jbw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='averageflatlanders'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1kr6jbw/what_an_iceberg_catalog_that_works/', 'stickied': False, 'url': 'https://dataengineeringcentral.substack.com/p/what-an-iceberg-catalog-that-works', 'subreddit_subscribers': 328156, 'created_utc': 1747751995.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-05-21T00:12:54.986+0000] {logging_mixin.py:151} INFO - None
[2025-05-21T00:12:54.987+0000] {python.py:194} INFO - Done. Returned value was: None
[2025-05-21T00:12:54.993+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=extract_reddit, execution_date=20250521T001253, start_date=20250521T001254, end_date=20250521T001254
[2025-05-21T00:12:55.020+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-05-21T00:12:55.029+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
